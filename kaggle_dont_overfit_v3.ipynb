{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC  \n",
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target      0      1      2      3      4      5      6      7  ...    \\\n",
       "0   0     1.0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276  ...     \n",
       "1   1     0.0  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  ...     \n",
       "2   2     1.0 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  ...     \n",
       "3   3     1.0  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  ...     \n",
       "4   4     1.0  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509  ...     \n",
       "\n",
       "     290    291    292    293    294    295    296    297    298    299  \n",
       "0  0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1 -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3 -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = pd.read_csv(\"C:/Users/mruna/Desktop/Kaggle/Dont_overfit/train.csv\")\n",
    "test_file = pd.read_csv(\"C:/Users/mruna/Desktop/Kaggle/Dont_overfit/test.csv\")\n",
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>124.500000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.023292</td>\n",
       "      <td>-0.026872</td>\n",
       "      <td>0.167404</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>-0.007304</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>0.126344</td>\n",
       "      <td>0.018436</td>\n",
       "      <td>-0.012092</td>\n",
       "      <td>-0.065720</td>\n",
       "      <td>-0.106112</td>\n",
       "      <td>0.046472</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>-0.128952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.312977</td>\n",
       "      <td>0.480963</td>\n",
       "      <td>0.998354</td>\n",
       "      <td>1.009314</td>\n",
       "      <td>1.021709</td>\n",
       "      <td>1.011751</td>\n",
       "      <td>1.035411</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>1.006657</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011416</td>\n",
       "      <td>0.972567</td>\n",
       "      <td>0.954229</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>1.057414</td>\n",
       "      <td>1.038389</td>\n",
       "      <td>0.967661</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>1.008099</td>\n",
       "      <td>0.971219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.319000</td>\n",
       "      <td>-2.931000</td>\n",
       "      <td>-2.477000</td>\n",
       "      <td>-2.359000</td>\n",
       "      <td>-2.566000</td>\n",
       "      <td>-2.845000</td>\n",
       "      <td>-2.976000</td>\n",
       "      <td>-3.444000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.804000</td>\n",
       "      <td>-2.443000</td>\n",
       "      <td>-2.757000</td>\n",
       "      <td>-2.466000</td>\n",
       "      <td>-3.287000</td>\n",
       "      <td>-3.072000</td>\n",
       "      <td>-2.634000</td>\n",
       "      <td>-2.776000</td>\n",
       "      <td>-3.211000</td>\n",
       "      <td>-3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>62.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.644750</td>\n",
       "      <td>-0.739750</td>\n",
       "      <td>-0.425250</td>\n",
       "      <td>-0.686500</td>\n",
       "      <td>-0.659000</td>\n",
       "      <td>-0.643750</td>\n",
       "      <td>-0.675000</td>\n",
       "      <td>-0.550750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617000</td>\n",
       "      <td>-0.510500</td>\n",
       "      <td>-0.535750</td>\n",
       "      <td>-0.657000</td>\n",
       "      <td>-0.818500</td>\n",
       "      <td>-0.821000</td>\n",
       "      <td>-0.605500</td>\n",
       "      <td>-0.751250</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>-0.754250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>124.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015500</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>-0.023000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>-0.021000</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.079500</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>-0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>186.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.620750</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.783250</td>\n",
       "      <td>0.766250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797250</td>\n",
       "      <td>0.804250</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.650250</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>0.794250</td>\n",
       "      <td>0.654250</td>\n",
       "      <td>0.503250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>249.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.567000</td>\n",
       "      <td>2.419000</td>\n",
       "      <td>3.392000</td>\n",
       "      <td>2.771000</td>\n",
       "      <td>2.901000</td>\n",
       "      <td>2.793000</td>\n",
       "      <td>2.546000</td>\n",
       "      <td>2.846000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.865000</td>\n",
       "      <td>2.801000</td>\n",
       "      <td>2.736000</td>\n",
       "      <td>2.596000</td>\n",
       "      <td>2.226000</td>\n",
       "      <td>3.131000</td>\n",
       "      <td>3.236000</td>\n",
       "      <td>2.626000</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      target           0           1           2           3  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
       "mean   124.500000    0.640000    0.023292   -0.026872    0.167404    0.001904   \n",
       "std     72.312977    0.480963    0.998354    1.009314    1.021709    1.011751   \n",
       "min      0.000000    0.000000   -2.319000   -2.931000   -2.477000   -2.359000   \n",
       "25%     62.250000    0.000000   -0.644750   -0.739750   -0.425250   -0.686500   \n",
       "50%    124.500000    1.000000   -0.015500    0.057000    0.184000   -0.016500   \n",
       "75%    186.750000    1.000000    0.677000    0.620750    0.805000    0.720000   \n",
       "max    249.000000    1.000000    2.567000    2.419000    3.392000    2.771000   \n",
       "\n",
       "                4           5           6           7     ...             290  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000     ...      250.000000   \n",
       "mean     0.001588   -0.007304    0.032052    0.078412     ...        0.044652   \n",
       "std      1.035411    0.955700    1.006657    0.939731     ...        1.011416   \n",
       "min     -2.566000   -2.845000   -2.976000   -3.444000     ...       -2.804000   \n",
       "25%     -0.659000   -0.643750   -0.675000   -0.550750     ...       -0.617000   \n",
       "50%     -0.023000    0.037500    0.060500    0.183500     ...        0.067500   \n",
       "75%      0.735000    0.660500    0.783250    0.766250     ...        0.797250   \n",
       "max      2.901000    2.793000    2.546000    2.846000     ...        2.865000   \n",
       "\n",
       "              291         292         293         294         295         296  \\\n",
       "count  250.000000  250.000000  250.000000  250.000000  250.000000  250.000000   \n",
       "mean     0.126344    0.018436   -0.012092   -0.065720   -0.106112    0.046472   \n",
       "std      0.972567    0.954229    0.960630    1.057414    1.038389    0.967661   \n",
       "min     -2.443000   -2.757000   -2.466000   -3.287000   -3.072000   -2.634000   \n",
       "25%     -0.510500   -0.535750   -0.657000   -0.818500   -0.821000   -0.605500   \n",
       "50%      0.091000    0.057500   -0.021000   -0.009000   -0.079500    0.009500   \n",
       "75%      0.804250    0.631500    0.650250    0.739500    0.493000    0.683000   \n",
       "max      2.801000    2.736000    2.596000    2.226000    3.131000    3.236000   \n",
       "\n",
       "              297         298         299  \n",
       "count  250.000000  250.000000  250.000000  \n",
       "mean     0.006452    0.009372   -0.128952  \n",
       "std      0.998984    1.008099    0.971219  \n",
       "min     -2.776000   -3.211000   -3.500000  \n",
       "25%     -0.751250   -0.550000   -0.754250  \n",
       "50%      0.005500   -0.009000   -0.132500  \n",
       "75%      0.794250    0.654250    0.503250  \n",
       "max      2.626000    3.530000    2.771000  \n",
       "\n",
       "[8 rows x 302 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGIhJREFUeJzt3Xm0XXV99/H3R8IgIgTMRYEQQ9ug\ngg8q5kHUZYuiFpzgsWoBh6h0xbY4PXVAravwtGLVaqkjrlQQsIoiigSrVYoDdQANFJFBSopIwpQw\nTxaJfp8/9r7kcN333pOYc86F+36tddc957eH3/fc3JzP/f322XunqpAkaaKHjLoASdLMZEBIkjoZ\nEJKkTgaEJKmTASFJ6mRASJI6GRB6wElydJJ/aR8vTFJJ5oy6rk0lyXeS/Fn7+OVJvrkJ931Jkv3a\nx/f9HDfRvt+V5FOban8aPQNCfUny6iQ/TXJ3kuuTHJdk7qjrmk6Sq5I8e4T9n5jkPRu7fVV9tqqe\nu6n6qao9q+o7G1tPT3/7JVk9Yd/vrao/+133rZnDgNC0krwFeD/wNmA7YF/g0cBZSbbYxH3NqJFA\nks1GXcOmMNN+rnpgMCA0pSTbAv8PeENV/VtV3VtVVwEvowmJVyTZOckvk+zQs92TktyYZPP2+WuT\nXJbkliTfSPLonnUryRFJrgCuaNs+nGRVktuTnJ/kGRtR+2eABcCZSe5M8va2/YvtKOi2JOck2bNn\nmxPb0dHXktwFPDPJI5Kc2dby4yTvSfK9nm0em+SsJDcnuTzJy9r2pcDLgbe3/Z85SZ3PSfKztp6P\nAelZ9urxvtI4Nsmadt2Lkjx+sn7a0dORSS4C7koyp2NEtVWSLyS5I8kFSZ4w4d/lDyb8bN6T5GHA\n14Gd2/7ubH8H7jdlleRF7ZTWre202eN6ll2V5K3ta7itrWGr/v91NQwGhKbzNGAr4Mu9jVV1J82b\nxHOq6lrgh8Cf9KxyGHBaVd2b5GDgXcCLgTHgP4BTJvRzMPAUYI/2+Y+BJwI7AJ8DvrihbyBV9Urg\nauCFVbVNVX2gXfR1YBGwI3AB8NkJmx4GHAM8HPge8HHgLuBRwJL2C4D2zfKstsYdgUOBTyTZs6qW\ntfv+QNv/CyfWmGQe8CXg3cA84L+Bp0/ykp4L/CGwOzAX+FPgpmn6ORR4PjC3qtZ17PMg4Ius/zl/\nZTzUJ1NVdwEHAte2/W3T/g70vq7daf6N30zzb/41mqDuHXG+DDgA2A3YC3j1VP1q+AwITWcecOMk\nby7XtcuheXM5FJq/dIFD2jaA1wF/X1WXtft5L/DE3lFEu/zmqvolQFX9S1XdVFXrqupDwJbAYzbF\nC6qqE6rqjqq6BzgaeEKS7XpWOaOqvl9VvwHupQm+o6rq7qq6FDipZ90XAFdV1afbWi+gecN/SZ/l\nPA+4tKpOq6p7gX8Crp9k3XtpQuuxQNqf53XT7P8jVbVq/Ofa4fyevv+R5o+BffusfSp/CvxrVZ3V\n7vuDwENp/uDore3aqroZOJPmDwLNIAaEpnMjMG+SOeyd2uUApwFPTbIzzV+5RTNSgGYq6sPtVMOt\nwM000yi79OxrVe+Ok7ylnZK6rd1mO9aH0UZLslmS9yX57yS3A1e1i3r33VvLGDBnQlvv40cDTxl/\nbW2tL6cZbfRj5979VXP1zFVdK1bVt4CP0YxobkiyrJ0CnErnvrqWt4G4uq3pd7Uz8IsJ+17F/f/N\ne4PwbmCbTdCvNiEDQtP5IXAPzfTQfdqplQOBswGq6lbgmzTTBocBp9T6SwWvAl5XVXN7vh5aVT/o\n2WX17PsZwJHtvravqrnAbfTMzW+AiZcrPoxmWuXZNKGzcLzbSbZZC6wD5ve07drzeBXw3QmvbZuq\n+otJ+p/out79taOvXSdbuao+UlVPBvakmWp62zT9TNd/b98PoXmd49NFdwNb96zbG3rT7fdamvAc\n3/f467pmmu00gxgQmlJV3UZzkPqjSQ5IsnmShTTz1quBz/Ss/jngVTRTMp/raf8k8M7xg8FJtkvy\n0im6fTjNm/JaYE6SvwGm+0t5MjcAvzdh3/cAN9G8+b13qo2r6tc0x1+OTrJ1ksfSvMZxXwV2T/LK\n9mezeZL/3XNAdmL/E/0rsGeSF7ejtDcyyeij3e9T2mMEdwH/A/y6z34m8+Sevt9M87M5t112IXBY\nO+o6APijnu1uAB4xYWqu16nA85Ps39b7lnbfP5hkfc1ABoSm1R7cfRfNPPLtwHk0fznv387jj1tO\nc/D3hqr6Sc/2p9N8TPbz7bTOxTSjj8l8g+ZA8n/RTFP8D9NPlUzm74F3t9M/bwVObvd5DXAp698M\np/J6mtHG9TSBeArNmx1VdQfNweNDaP5qvp7mtW7Zbns8sEfb/1cm7riqbgReCryPJrQWAd+fpI5t\ngX8Gbmlfw000/ybT9jOFM2iOF9wCvBJ4cXvMAOBNwAuB8Wmz+/ZbVT9rfw5Xtn3eb1qqqi4HXgF8\nlGYa8oU0Hxb41QbUphGLNwySNkyS9wOPqqol064sPYA5gpCmkeY8h73a8xD2AQ4HTh91XdKgeXal\nNL2H00yn7AysAT5EMzUjPag5xSRJ6uQUkySp0wN6imnevHm1cOHCUZchSQ8o559//o1VNTbdeg/o\ngFi4cCErVqwYdRmS9ICS5BfTr+UUkyRpEgaEJKmTASFJ6mRASJI6GRCSpE4GhCSp08ACIskJ7b1z\nL57Q/oY09+29JMkHetrfmWRlu+yPB1WXJKk/gzwP4kSau1+dPN6Q5Jk0N2vZq6ruSbJj274HzeWS\n96S53s2/J9m9vRa/JGkEBjaCqKpzaG4t2esvgPeN30Ogqta07QcBn6+qe6rq58BKYJ9B1SZJmt6w\nz6TeHXhGkmNobgLz1qr6Mc19antv3LKa+9+79j5JlgJLARYsWDDYaqURuvpv/9eoS9AMtOBvfjq0\nvoZ9kHoOsD2wL829dE9t71Xbda/hzsvMVtWyqlpcVYvHxqa9lIgkaSMNOyBWA1+uxo+A3wDz2vbe\nG7X33jhdkjQCww6IrwDPAkiyO7AFzf1qlwOHJNkyyW409+X90ZBrkyT1GNgxiCSnAPsB85KsBo4C\nTgBOaD/6+itgSTV3LLokyak0N5FfBxzhJ5gkabQGFhBVdegki14xyfrHAMcMqh5J0obxTGpJUicD\nQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicD\nQpLUyYCQJHUyICRJnQYWEElOSLKmvXvcxGVvTVJJ5rXPk+QjSVYmuSjJ3oOqS5LUn0GOIE4EDpjY\nmGRX4DnA1T3NB9Lch3oRsBQ4boB1SZL6MLCAqKpzgJs7Fh0LvB2onraDgJOrcS4wN8lOg6pNkjS9\noR6DSPIi4Jqq+smERbsAq3qer27bJEkjMmdYHSXZGvhr4LldizvaqqONJEtppqFYsGDBJqtPknR/\nwxxB/D6wG/CTJFcB84ELkjyKZsSwa8+684Fru3ZSVcuqanFVLR4bGxtwyZI0ew0tIKrqp1W1Y1Ut\nrKqFNKGwd1VdDywHXtV+mmlf4Laqum5YtUmSftsgP+Z6CvBD4DFJVic5fIrVvwZcCawE/hn4y0HV\nJUnqz8COQVTVodMsX9jzuIAjBlWLJGnDeSa1JKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiS\nOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSp0yBvOXpCkjVJ\nLu5p+4ckP0tyUZLTk8ztWfbOJCuTXJ7kjwdVlySpP4McQZwIHDCh7Szg8VW1F/BfwDsBkuwBHALs\n2W7ziSSbDbA2SdI0BhYQVXUOcPOEtm9W1br26bnA/PbxQcDnq+qeqvo5sBLYZ1C1SZKmN8pjEK8F\nvt4+3gVY1bNsddv2W5IsTbIiyYq1a9cOuERJmr1GEhBJ/hpYB3x2vKljteratqqWVdXiqlo8NjY2\nqBIladabM+wOkywBXgDsX1XjIbAa2LVntfnAtcOuTZK03lBHEEkOAI4EXlRVd/csWg4ckmTLJLsB\ni4AfDbM2SdL9DWwEkeQUYD9gXpLVwFE0n1raEjgrCcC5VfXnVXVJklOBS2mmno6oql8PqjZJ0vQG\nFhBVdWhH8/FTrH8McMyg6pEkbRjPpJYkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS\n1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUaWEAkOSHJmiQX97Tt\nkOSsJFe037dv25PkI0lWJrkoyd6DqkuS1J9BjiBOBA6Y0PYO4OyqWgSc3T4HOBBY1H4tBY4bYF2S\npD4MLCCq6hzg5gnNBwEntY9PAg7uaT+5GucCc5PsNKjaJEnTG/YxiEdW1XUA7fcd2/ZdgFU9661u\n235LkqVJViRZsXbt2oEWK0mz2Uw5SJ2OtupasaqWVdXiqlo8NjY24LIkafYadkDcMD511H5f07av\nBnbtWW8+cO2Qa5Mk9Rh2QCwHlrSPlwBn9LS/qv00077AbeNTUZKk0ZgzqB0nOQXYD5iXZDVwFPA+\n4NQkhwNXAy9tV/8a8DxgJXA38JpB1SVJ6s/AAqKqDp1k0f4d6xZwxKBqkSRtuJlykFqSNMMYEJKk\nTgaEJKmTASFJ6tRXQCQ5u582SdKDx5SfYkqyFbA1zUdVt2f9Gc/bAjsPuDZJ0ghN9zHX1wFvpgmD\n81kfELcDHx9gXZKkEZsyIKrqw8CHk7yhqj46pJqG6slvO3nUJWgGOv8fXjXqEqSR6+tEuar6aJKn\nAQt7t6kq310l6UGqr4BI8hng94ELgV+3zQUYEJL0INXvpTYWA3u0l8SQJM0C/Z4HcTHwqEEWIkma\nWfodQcwDLk3yI+Ce8caqetFAqpIkjVy/AXH0IIuQJM08/X6K6buDLkSSNLP0+ymmO1h/j+gtgM2B\nu6pq20EVJkkarX5HEA/vfZ7kYGCfgVQkSZoRNupqrlX1FeBZG9tpkv+b5JIkFyc5JclWSXZLcl6S\nK5J8IckWG7t/SdLvrt8pphf3PH0IzXkRG3VORJJdgDfSnFfxyySnAofQ3JP62Kr6fJJPAocDx21M\nH5Kk312/n2J6Yc/jdcBVwEG/Y78PTXIvzdVir6MZkRzWLj+J5pNTBoQkjUi/xyBes6k6rKprknwQ\nuBr4JfBNmivF3lpV69rVVgO7dG2fZCmwFGDBggWbqixJ0gT93jBofpLTk6xJckOSLyWZvzEdtveV\nOAjYjeYy4g8DDuxYtXMKq6qWVdXiqlo8Nja2MSVIkvrQ70HqTwPLad7QdwHObNs2xrOBn1fV2qq6\nF/gy8DRgbpLxEc184NqN3L8kaRPoNyDGqurTVbWu/ToR2Ng/368G9k2ydZIA+wOXAt8GXtKuswQ4\nYyP3L0naBPoNiBuTvCLJZu3XK4CbNqbDqjoPOA24APhpW8My4Ejgr5KsBB4BHL8x+5ckbRr9forp\ntcDHgGNpjg38ANjoA9dVdRRw1ITmK/HkO0maMfoNiL8DllTVLQBJdgA+SBMckqQHoX6nmPYaDweA\nqroZeNJgSpIkzQT9BsRD2o+nAveNIPodfUiSHoD6fZP/EPCDJKfRHIN4GXDMwKqSJI1cv2dSn5xk\nBc3lMAK8uKouHWhlkqSR6nuaqA0EQ0GSZomNuty3JOnBz4CQJHUyICRJnQwISVInA0KS1MmAkCR1\nMiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdRhIQSeYmOS3Jz5JcluSpSXZIclaSK9rv20+/J0nSoIxq\nBPFh4N+q6rHAE4DLgHcAZ1fVIuDs9rkkaUSGHhBJtgX+EDgeoKp+VVW3AgcBJ7WrnQQcPOzaJEnr\njWIE8XvAWuDTSf4zyaeSPAx4ZFVdB9B+37Fr4yRLk6xIsmLt2rXDq1qSZplRBMQcYG/guKp6EnAX\nGzCdVFXLqmpxVS0eGxsbVI2SNOuNIiBWA6ur6rz2+Wk0gXFDkp0A2u9rRlCbJKk19ICoquuBVUke\n0zbtT3OnuuXAkrZtCXDGsGuTJK3X9y1HN7E3AJ9NsgVwJfAamrA6NcnhwNXAS0dUmySJEQVEVV0I\nLO5YtP+wa5EkdfNMaklSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS\n1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUqeRBUSSzZL8Z5Kvts93S3JekiuSfKG9\nHakkaURGOYJ4E3BZz/P3A8dW1SLgFuDwkVQlSQJGFBBJ5gPPBz7VPg/wLOC0dpWTgINHUZskqTGq\nEcQ/AW8HftM+fwRwa1Wta5+vBnbp2jDJ0iQrkqxYu3bt4CuVpFlq6AGR5AXAmqo6v7e5Y9Xq2r6q\nllXV4qpaPDY2NpAaJUkwZwR9Ph14UZLnAVsB29KMKOYmmdOOIuYD146gNklSa+gjiKp6Z1XNr6qF\nwCHAt6rq5cC3gZe0qy0Bzhh2bZKk9WbSeRBHAn+VZCXNMYnjR1yPJM1qo5hiuk9VfQf4Tvv4SmCf\nUdYjSVpvJo0gJEkziAEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRA\nSJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqdPQAyLJrkm+neSyJJckeVPbvkOSs5Jc0X7f\nfti1SZLWG8UIYh3wlqp6HLAvcESSPYB3AGdX1SLg7Pa5JGlEhh4QVXVdVV3QPr4DuAzYBTgIOKld\n7STg4GHXJklab6THIJIsBJ4EnAc8sqqugyZEgB0n2WZpkhVJVqxdu3ZYpUrSrDOygEiyDfAl4M1V\ndXu/21XVsqpaXFWLx8bGBlegJM1yIwmIJJvThMNnq+rLbfMNSXZql+8ErBlFbZKkxig+xRTgeOCy\nqvrHnkXLgSXt4yXAGcOuTZK03pwR9Pl04JXAT5Nc2La9C3gfcGqSw4GrgZeOoDZJUmvoAVFV3wMy\nyeL9h1mLJGlynkktSepkQEiSOhkQkqROBoQkqZMBIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6\nGRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqNOMCIskBSS5PsjLJO0ZdjyTNVjMqIJJs\nBnwcOBDYAzg0yR6jrUqSZqcZFRDAPsDKqrqyqn4FfB44aMQ1SdKsNGfUBUywC7Cq5/lq4Cm9KyRZ\nCixtn96Z5PIh1TYbzANuHHURM0E+uGTUJej+/N0cd1Q2xV4e3c9KMy0gul553e9J1TJg2XDKmV2S\nrKiqxaOuQ5rI383RmGlTTKuBXXuezweuHVEtkjSrzbSA+DGwKMluSbYADgGWj7gmSZqVZtQUU1Wt\nS/J64BvAZsAJVXXJiMuaTZy600zl7+YIpKqmX0uSNOvMtCkmSdIMYUBIkjoZELPQdJczSbJlki+0\ny89LsnD4VWq2SXJCkjVJLp5keZJ8pP29vCjJ3sOucbYxIGaZPi9ncjhwS1X9AXAs8P7hVqlZ6kTg\ngCmWHwgsar+WAscNoaZZzYCYffq5nMlBwEnt49OA/ZNsktM3pclU1TnAzVOschBwcjXOBeYm2Wk4\n1c1OBsTs03U5k10mW6eq1gG3AY8YSnXS5Pr53dUmZEDMPtNezqTPdaRh8/dyyAyI2aefy5nct06S\nOcB2TD30l4bBS/EMmQEx+/RzOZPlwPjlTF8CfKs8o1Kjtxx4Vftppn2B26rqulEX9WA2oy61ocGb\n7HImSf4WWFFVy4Hjgc8kWUkzcjhkdBVrtkhyCrAfMC/JauAoYHOAqvok8DXgecBK4G7gNaOpdPbw\nUhuSpE5OMUmSOhkQkqROBoQkqZMBIUnqZEBIkjoZENIUksxN8pdD6Ge/JE8bdD/ShjAgpKnNBfoO\niPYkro35f7UfYEBoRvE8CGkKScavdns58G1gL2B7mhO43l1VZ7T3y/h6u/ypwMHAs4EjaS4FcQVw\nT1W9PskY8ElgQdvFm4FrgHOBXwNrgTdU1X8M4/VJUzEgpCm0b/5frarHt9el2rqqbk8yj+ZNfRHw\naOBK4GlVdW6SnYEfAHsDdwDfAn7SBsTngE9U1feSLAC+UVWPS3I0cGdVfXDYr1GajJfakPoX4L1J\n/hD4Dc2lph/ZLvtFe48CaO658d2quhkgyReB3dtlzwb26Lm9xrZJHj6M4qUNZUBI/Xs5MAY8uaru\nTXIVsFW77K6e9aa6udJDgKdW1S97G70fk2YiD1JLU7sDGP8LfztgTRsOz6SZWuryI+CPkmzfTkv9\nSc+ybwKvH3+S5Ikd/UgzggEhTaGqbgK+n+Ri4InA4iQraEYTP5tkm2uA9wLnAf8OXEpzVz6AN7b7\nuCjJpcCft+1nAv8nyYVJnjGwFyRtAA9SSwOQZJuqurMdQZxOc1n100ddl7QhHEFIg3F0kguBi4Gf\nA18ZcT3SBnMEIUnq5AhCktTJgJAkdTIgJEmdDAhJUicDQpLU6f8DQ2juaEIqowIAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269a75c3400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_file['target']\n",
    "sns.countplot(x=\"target\",data=train_file)\n",
    "plt.title('Overall target distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0    1.000000 -0.003871 -0.010487 -0.047819  0.013967  0.070091 -0.022537   \n",
      "1   -0.003871  1.000000  0.013573 -0.018884  0.086743 -0.028023 -0.032914   \n",
      "2   -0.010487  0.013573  1.000000 -0.031620  0.088980 -0.050191  0.024674   \n",
      "3   -0.047819 -0.018884 -0.031620  1.000000  0.025255  0.172058  0.157954   \n",
      "4    0.013967  0.086743  0.088980  0.025255  1.000000 -0.013072 -0.030131   \n",
      "5    0.070091 -0.028023 -0.050191  0.172058 -0.013072  1.000000 -0.002426   \n",
      "6   -0.022537 -0.032914  0.024674  0.157954 -0.030131 -0.002426  1.000000   \n",
      "7    0.002832 -0.066416  0.035260 -0.018158  0.037315  0.087932 -0.035602   \n",
      "8   -0.060031 -0.027201 -0.000094  0.058736 -0.186559  0.013425 -0.043699   \n",
      "9   -0.052356  0.035512  0.009949 -0.087360  0.031577  0.055556 -0.014648   \n",
      "10   0.059263  0.020358  0.015158  0.095171 -0.008063  0.022024  0.139278   \n",
      "11  -0.014094  0.003761 -0.029568  0.007442  0.007968 -0.010794 -0.199293   \n",
      "12  -0.060538  0.027094  0.097398  0.045094  0.132275  0.040214 -0.077901   \n",
      "13   0.049368 -0.111360 -0.073799 -0.041555 -0.054940  0.053074 -0.028456   \n",
      "14  -0.017255 -0.030637 -0.000265 -0.031459  0.000499  0.000897 -0.018627   \n",
      "15  -0.047828 -0.084130  0.033148 -0.000283 -0.090977  0.084578  0.045245   \n",
      "16  -0.031047  0.023147 -0.054838 -0.065051  0.007262  0.051093  0.015405   \n",
      "17   0.013264 -0.115217 -0.019571 -0.025095 -0.096988 -0.131270  0.030455   \n",
      "18  -0.043416  0.065643 -0.078429  0.022526 -0.081156 -0.088878 -0.032627   \n",
      "19  -0.005945  0.051571 -0.015709 -0.069960 -0.058581  0.062958 -0.019670   \n",
      "20  -0.000825  0.055748  0.007613  0.001301  0.052647  0.037632 -0.090969   \n",
      "21   0.030775 -0.084465  0.031090 -0.045099  0.003676  0.099301 -0.057341   \n",
      "22  -0.031150  0.039373 -0.018003  0.040782  0.109518  0.041747 -0.068222   \n",
      "23  -0.055822 -0.033250  0.115579 -0.000773  0.066811 -0.040516 -0.020787   \n",
      "24  -0.030686  0.069004  0.002037 -0.058211  0.008885  0.027247  0.028545   \n",
      "25  -0.019718  0.105601  0.081629 -0.036736  0.077538 -0.109747  0.020797   \n",
      "26  -0.005644  0.011806  0.063708  0.081253 -0.040955  0.063418  0.116898   \n",
      "27   0.027852 -0.031546  0.135030  0.094281 -0.083632  0.035137  0.024613   \n",
      "28   0.035622 -0.134291  0.021411 -0.072850 -0.064853 -0.032448  0.041625   \n",
      "29  -0.008151 -0.058598 -0.066181  0.055668  0.045321  0.007844 -0.096269   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "270 -0.013818  0.026795 -0.125342  0.002538 -0.026924 -0.023682  0.024495   \n",
      "271 -0.022710 -0.020315  0.074181  0.103659  0.116022  0.039628  0.039890   \n",
      "272  0.007584 -0.006171 -0.160163  0.134583  0.032491  0.033679 -0.003165   \n",
      "273 -0.020473 -0.146324 -0.031669  0.030648 -0.023327 -0.064961 -0.086702   \n",
      "274  0.075098 -0.015831  0.021250 -0.048846 -0.018470 -0.062320  0.066603   \n",
      "275  0.054375 -0.049975  0.046178  0.068337 -0.013100  0.003703  0.036888   \n",
      "276 -0.028977 -0.031120  0.040289 -0.086846  0.021474  0.001804 -0.013716   \n",
      "277 -0.041199  0.070214 -0.027123  0.000727  0.077814 -0.019020 -0.082993   \n",
      "278  0.022243 -0.016917 -0.073515 -0.058637 -0.047860 -0.023463 -0.056725   \n",
      "279 -0.102684  0.032828 -0.012218  0.000049  0.016064 -0.025639  0.094583   \n",
      "280 -0.024006  0.098995 -0.036694  0.010798 -0.040175 -0.003489 -0.021445   \n",
      "281  0.037312  0.119581  0.080078 -0.029653 -0.021688 -0.044853 -0.019179   \n",
      "282  0.135009 -0.096928 -0.010019 -0.001238  0.031517 -0.047173  0.067782   \n",
      "283  0.029303  0.066795 -0.122932 -0.093418 -0.054424  0.082530 -0.071567   \n",
      "284  0.031001 -0.047459  0.059758  0.015668 -0.010751 -0.052762 -0.050025   \n",
      "285  0.033796  0.058864 -0.066441 -0.023216 -0.048290  0.042365  0.051614   \n",
      "286  0.164564  0.041791  0.003450  0.042055 -0.093875  0.090503  0.118022   \n",
      "287  0.012922  0.096635  0.073678  0.030620  0.094702  0.015096 -0.135902   \n",
      "288 -0.027903 -0.131251 -0.090725  0.048973  0.011539  0.020335  0.013871   \n",
      "289  0.034457 -0.070546  0.046566 -0.028037  0.083630  0.102683 -0.143277   \n",
      "290 -0.023230 -0.006300 -0.111172 -0.114505  0.096056  0.027638 -0.026921   \n",
      "291  0.053416 -0.077365 -0.027842  0.029415  0.056339 -0.095449  0.133380   \n",
      "292 -0.143668 -0.021583 -0.013653  0.129069 -0.091165  0.002309 -0.024050   \n",
      "293 -0.007530 -0.054171 -0.009691  0.096159  0.073098  0.049059 -0.012804   \n",
      "294 -0.060824 -0.046174 -0.051292 -0.027793  0.158532 -0.032129  0.084606   \n",
      "295 -0.024839  0.042820 -0.028690 -0.005016 -0.050318  0.026868 -0.023192   \n",
      "296 -0.051288 -0.127499 -0.071835  0.034551 -0.030017  0.070294  0.036903   \n",
      "297  0.029143  0.065883  0.069395  0.089963  0.029747  0.069243  0.037912   \n",
      "298  0.065951  0.055470  0.083946 -0.066438 -0.008517 -0.048009  0.171640   \n",
      "299  0.038523 -0.056612  0.025507 -0.010770 -0.040654 -0.084178 -0.004655   \n",
      "\n",
      "            7         8         9    ...          290       291       292  \\\n",
      "0    0.002832 -0.060031 -0.052356    ...    -0.023230  0.053416 -0.143668   \n",
      "1   -0.066416 -0.027201  0.035512    ...    -0.006300 -0.077365 -0.021583   \n",
      "2    0.035260 -0.000094  0.009949    ...    -0.111172 -0.027842 -0.013653   \n",
      "3   -0.018158  0.058736 -0.087360    ...    -0.114505  0.029415  0.129069   \n",
      "4    0.037315 -0.186559  0.031577    ...     0.096056  0.056339 -0.091165   \n",
      "5    0.087932  0.013425  0.055556    ...     0.027638 -0.095449  0.002309   \n",
      "6   -0.035602 -0.043699 -0.014648    ...    -0.026921  0.133380 -0.024050   \n",
      "7    1.000000  0.016259 -0.001542    ...     0.002158  0.109255 -0.010870   \n",
      "8    0.016259  1.000000  0.059123    ...    -0.081837 -0.035636 -0.008774   \n",
      "9   -0.001542  0.059123  1.000000    ...     0.118215  0.018054  0.068017   \n",
      "10  -0.072135 -0.051984 -0.066612    ...     0.041851 -0.090508 -0.066538   \n",
      "11   0.000696  0.027498  0.048126    ...    -0.016465 -0.074872  0.048479   \n",
      "12  -0.029012 -0.022826  0.093749    ...    -0.044404  0.060939  0.085301   \n",
      "13  -0.129851 -0.073796  0.087873    ...     0.067172 -0.046984 -0.039332   \n",
      "14   0.003211 -0.041600  0.011040    ...     0.050299 -0.062841 -0.033369   \n",
      "15   0.014177 -0.093867 -0.098303    ...    -0.020125  0.041200  0.016527   \n",
      "16   0.019315 -0.082160 -0.047054    ...    -0.047543 -0.004746 -0.077549   \n",
      "17   0.039841  0.034296  0.039255    ...     0.058518 -0.067853 -0.067064   \n",
      "18  -0.064447 -0.019739 -0.063121    ...    -0.002533  0.035025  0.008584   \n",
      "19   0.066830  0.045290 -0.094595    ...    -0.063456 -0.022331  0.028766   \n",
      "20  -0.037427  0.060797  0.066107    ...    -0.057285 -0.029771  0.066776   \n",
      "21   0.058795 -0.016320 -0.038130    ...     0.080430  0.048942 -0.049956   \n",
      "22  -0.128597  0.029398 -0.084042    ...     0.096401 -0.163088  0.075764   \n",
      "23   0.001874  0.015994  0.003930    ...    -0.010177 -0.059642  0.053468   \n",
      "24   0.020165 -0.091723  0.073740    ...     0.105075 -0.015667  0.020669   \n",
      "25  -0.073241 -0.026322  0.004869    ...     0.000259  0.023866  0.034298   \n",
      "26   0.029705  0.025675  0.031273    ...     0.019259 -0.031011 -0.007975   \n",
      "27   0.016562  0.003313 -0.044160    ...    -0.079914  0.022919  0.121734   \n",
      "28  -0.022672 -0.060709  0.030479    ...     0.086177  0.056256 -0.042673   \n",
      "29   0.074185 -0.028484 -0.086451    ...     0.016580 -0.050236 -0.015755   \n",
      "..        ...       ...       ...    ...          ...       ...       ...   \n",
      "270 -0.085310  0.029798  0.091478    ...    -0.060298 -0.004411  0.076581   \n",
      "271 -0.047192 -0.026331 -0.024132    ...    -0.008536  0.016448  0.027192   \n",
      "272 -0.029143  0.023825  0.095942    ...     0.093476 -0.010894 -0.025662   \n",
      "273 -0.032813 -0.058072  0.002427    ...     0.026530 -0.088661  0.069209   \n",
      "274  0.004536 -0.084231 -0.004351    ...     0.048766 -0.020406  0.059892   \n",
      "275  0.076550 -0.022534 -0.209210    ...    -0.043427  0.035012 -0.065120   \n",
      "276 -0.099319  0.015097  0.061177    ...     0.011443 -0.025595  0.048191   \n",
      "277 -0.004380 -0.068849  0.005829    ...     0.122938 -0.035489  0.069662   \n",
      "278 -0.037637  0.035750 -0.001927    ...     0.126097 -0.060095 -0.052067   \n",
      "279  0.105528  0.031182 -0.104338    ...     0.028292  0.014443 -0.032497   \n",
      "280 -0.009275 -0.028809 -0.046256    ...     0.041386 -0.038085  0.006747   \n",
      "281 -0.093149 -0.008964  0.064582    ...    -0.087108  0.003421  0.062770   \n",
      "282  0.038228 -0.127083  0.046999    ...     0.018572  0.027043  0.012581   \n",
      "283  0.042598 -0.008289  0.031858    ...    -0.037522 -0.071252  0.129363   \n",
      "284 -0.011766 -0.033104 -0.007260    ...    -0.041029  0.030379  0.029664   \n",
      "285  0.026825  0.039401  0.116915    ...    -0.009558  0.051828  0.028306   \n",
      "286  0.031797 -0.006337  0.070715    ...    -0.022406  0.066551 -0.028410   \n",
      "287  0.065175  0.121365  0.032133    ...    -0.012988 -0.055725 -0.119592   \n",
      "288 -0.068548  0.004411  0.048638    ...     0.039070  0.052562 -0.083373   \n",
      "289  0.071279  0.028900  0.069416    ...     0.045336  0.035585  0.126121   \n",
      "290  0.002158 -0.081837  0.118215    ...     1.000000 -0.080239 -0.020013   \n",
      "291  0.109255 -0.035636  0.018054    ...    -0.080239  1.000000 -0.114165   \n",
      "292 -0.010870 -0.008774  0.068017    ...    -0.020013 -0.114165  1.000000   \n",
      "293  0.010266  0.027022  0.034661    ...     0.065859  0.022781  0.027094   \n",
      "294  0.071022 -0.008862  0.091779    ...    -0.161654  0.040141 -0.124483   \n",
      "295  0.032648  0.017297  0.096561    ...    -0.030738 -0.020251  0.015083   \n",
      "296 -0.006235 -0.071254 -0.041897    ...     0.016047  0.042079 -0.045879   \n",
      "297  0.014628  0.009224 -0.112156    ...     0.048496  0.066474 -0.025382   \n",
      "298 -0.031094 -0.000516 -0.005721    ...    -0.125556  0.087400  0.008096   \n",
      "299 -0.122393  0.011950  0.005209    ...     0.082621  0.017763  0.126105   \n",
      "\n",
      "          293       294       295       296       297       298       299  \n",
      "0   -0.007530 -0.060824 -0.024839 -0.051288  0.029143  0.065951  0.038523  \n",
      "1   -0.054171 -0.046174  0.042820 -0.127499  0.065883  0.055470 -0.056612  \n",
      "2   -0.009691 -0.051292 -0.028690 -0.071835  0.069395  0.083946  0.025507  \n",
      "3    0.096159 -0.027793 -0.005016  0.034551  0.089963 -0.066438 -0.010770  \n",
      "4    0.073098  0.158532 -0.050318 -0.030017  0.029747 -0.008517 -0.040654  \n",
      "5    0.049059 -0.032129  0.026868  0.070294  0.069243 -0.048009 -0.084178  \n",
      "6   -0.012804  0.084606 -0.023192  0.036903  0.037912  0.171640 -0.004655  \n",
      "7    0.010266  0.071022  0.032648 -0.006235  0.014628 -0.031094 -0.122393  \n",
      "8    0.027022 -0.008862  0.017297 -0.071254  0.009224 -0.000516  0.011950  \n",
      "9    0.034661  0.091779  0.096561 -0.041897 -0.112156 -0.005721  0.005209  \n",
      "10  -0.061096 -0.022770  0.004286  0.012922 -0.096935 -0.027523  0.089395  \n",
      "11  -0.080823 -0.011480  0.029916  0.021926 -0.091752 -0.107691  0.056845  \n",
      "12   0.091656  0.018026  0.000463  0.095595 -0.033143 -0.122686  0.027726  \n",
      "13  -0.020716 -0.039562 -0.062818 -0.043391  0.064835 -0.047400  0.000047  \n",
      "14  -0.008123  0.085024  0.025472  0.019281 -0.096848 -0.058028 -0.021498  \n",
      "15   0.080148 -0.067548 -0.058256  0.079084  0.057779  0.021571  0.000667  \n",
      "16   0.012209  0.101772 -0.033328  0.062210 -0.032590 -0.002240 -0.022099  \n",
      "17   0.047982  0.034879 -0.122605 -0.000236  0.053383  0.068385 -0.111602  \n",
      "18   0.124090  0.008737 -0.005406 -0.099695  0.094738 -0.026347  0.059311  \n",
      "19  -0.051663 -0.005981  0.002869 -0.031731  0.098549  0.044596 -0.090319  \n",
      "20  -0.078849  0.125156  0.107947  0.012653  0.027744 -0.049494  0.080048  \n",
      "21  -0.079083 -0.010715 -0.045315 -0.091541 -0.005069 -0.001075  0.002890  \n",
      "22  -0.013203 -0.054997 -0.003361  0.001706  0.060292 -0.037747 -0.050302  \n",
      "23   0.001489 -0.032424  0.009173 -0.022030 -0.012929 -0.013223 -0.068532  \n",
      "24   0.008645  0.033071  0.036107 -0.115851  0.016726 -0.080493 -0.066220  \n",
      "25   0.006116  0.093629  0.067894  0.002066  0.066989 -0.017905  0.110874  \n",
      "26   0.046786  0.004116  0.072794 -0.016036 -0.019523  0.056156  0.021063  \n",
      "27   0.067409  0.016727  0.040597  0.013379 -0.041923 -0.016833  0.107508  \n",
      "28   0.090732 -0.057705 -0.004402 -0.094865 -0.008893  0.010796 -0.020644  \n",
      "29   0.131867 -0.030125 -0.022230 -0.009884  0.161778 -0.141259  0.092846  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "270  0.075086 -0.056955 -0.065618 -0.036337 -0.034799 -0.030726  0.026477  \n",
      "271 -0.034761 -0.041243  0.024534  0.084293  0.001520 -0.013832  0.102211  \n",
      "272 -0.076839 -0.007824 -0.023935 -0.007085  0.098548 -0.060075  0.042014  \n",
      "273 -0.078902 -0.032598 -0.026823 -0.044852  0.020186 -0.008378 -0.040816  \n",
      "274  0.015857 -0.073589  0.046715 -0.128732 -0.063394 -0.029994  0.065657  \n",
      "275 -0.077779 -0.065888 -0.064713 -0.054901  0.098359  0.084593  0.024700  \n",
      "276  0.025829 -0.023381  0.071940  0.030464 -0.031393 -0.101621  0.157711  \n",
      "277  0.041885 -0.035982  0.064799 -0.002316 -0.000300 -0.048212  0.009138  \n",
      "278  0.062113 -0.047016 -0.036317 -0.080538 -0.063531  0.042847  0.087687  \n",
      "279 -0.087180 -0.027039 -0.017401  0.048129  0.101069  0.095338 -0.091398  \n",
      "280 -0.045962  0.006860  0.001940  0.061356  0.015178  0.021407 -0.023283  \n",
      "281 -0.029372  0.037314 -0.020929 -0.058629  0.057701  0.009097 -0.062859  \n",
      "282  0.120003  0.064526 -0.041584 -0.025202  0.051016  0.005093  0.036934  \n",
      "283  0.026577  0.068396 -0.028941 -0.054329 -0.043626 -0.105624 -0.023983  \n",
      "284  0.085798 -0.053926  0.085140 -0.036471 -0.020147  0.024573  0.022451  \n",
      "285 -0.051694 -0.111162 -0.021471 -0.011472 -0.127372 -0.028709  0.050203  \n",
      "286 -0.048638 -0.022079  0.000373 -0.022548  0.011568 -0.015270  0.037285  \n",
      "287 -0.069974 -0.017616  0.070608 -0.073599 -0.057366  0.029770 -0.069244  \n",
      "288  0.077909 -0.030012 -0.154858  0.005955 -0.046789  0.129112  0.071017  \n",
      "289  0.041402  0.016806 -0.068796 -0.092074  0.076721 -0.115568  0.001902  \n",
      "290  0.065859 -0.161654 -0.030738  0.016047  0.048496 -0.125556  0.082621  \n",
      "291  0.022781  0.040141 -0.020251  0.042079  0.066474  0.087400  0.017763  \n",
      "292  0.027094 -0.124483  0.015083 -0.045879 -0.025382  0.008096  0.126105  \n",
      "293  1.000000 -0.005332 -0.139025 -0.011766  0.021148 -0.005422  0.012255  \n",
      "294 -0.005332  1.000000 -0.015920  0.125693 -0.103255 -0.063242 -0.081196  \n",
      "295 -0.139025 -0.015920  1.000000 -0.072721 -0.036572 -0.034341  0.097052  \n",
      "296 -0.011766  0.125693 -0.072721  1.000000 -0.002007  0.056297  0.040264  \n",
      "297  0.021148 -0.103255 -0.036572 -0.002007  1.000000  0.039793 -0.141078  \n",
      "298 -0.005422 -0.063242 -0.034341  0.056297  0.039793  1.000000 -0.092017  \n",
      "299  0.012255 -0.081196  0.097052  0.040264 -0.141078 -0.092017  1.000000  \n",
      "\n",
      "[300 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "train_file_corr = train_file.drop(['id', 'target'], axis = 1).corr()\n",
    "print(train_file_corr)\n",
    "# sns.heatmap(train_file_corr, xticklabels = train_file_corr.columns, yticklabels = train_file_corr.columns)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file_corr[train_file_corr['1'] >= 0.5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.098</td>\n",
       "      <td>2.165</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.614</td>\n",
       "      <td>1.309</td>\n",
       "      <td>-0.455</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.276</td>\n",
       "      <td>-2.246</td>\n",
       "      <td>1.825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.097</td>\n",
       "      <td>1.051</td>\n",
       "      <td>-0.414</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-1.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.081</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.428</td>\n",
       "      <td>0.317</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>-1.695</td>\n",
       "      <td>-1.257</td>\n",
       "      <td>1.359</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>-1.624</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>-1.099</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.263</td>\n",
       "      <td>-1.222</td>\n",
       "      <td>0.726</td>\n",
       "      <td>1.444</td>\n",
       "      <td>-1.165</td>\n",
       "      <td>-1.544</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-1.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.067</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-1.637</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.725</td>\n",
       "      <td>-1.035</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404</td>\n",
       "      <td>0.640</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>-0.966</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.254</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.347</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.511</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.225</td>\n",
       "      <td>1.594</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.509</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>2.198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.415</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>-1.006</td>\n",
       "      <td>1.378</td>\n",
       "      <td>1.246</td>\n",
       "      <td>1.478</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9  \\\n",
       "0 -0.098  2.165  0.681 -0.614  1.309 -0.455 -0.236  0.276 -2.246  1.825   \n",
       "1  1.081 -0.973 -0.383  0.326 -0.428  0.317  1.172  0.352  0.004 -0.291   \n",
       "2 -0.523 -0.089 -0.348  0.148 -0.022  0.404 -0.023 -0.172  0.137  0.183   \n",
       "3  0.067 -0.021  0.392 -1.637 -0.446 -0.725 -1.035  0.834  0.503  0.274   \n",
       "4  2.347 -0.831  0.511 -0.021  1.225  1.594  0.585  1.509 -0.012  2.198   \n",
       "\n",
       "   ...      290    291    292    293    294    295    296    297    298    299  \n",
       "0  ...    0.867  1.347  0.504 -0.649  0.672 -2.097  1.051 -0.414  1.038 -1.065  \n",
       "1  ...   -0.165 -1.695 -1.257  1.359 -0.808 -1.624 -0.458 -1.099 -0.936  0.973  \n",
       "2  ...    0.013  0.263 -1.222  0.726  1.444 -1.165 -1.544  0.004  0.800 -1.211  \n",
       "3  ...   -0.404  0.640 -0.595 -0.966  0.900  0.467 -0.562 -0.254 -0.533  0.238  \n",
       "4  ...    0.898  0.134  2.415 -0.996 -1.006  1.378  1.246  1.478  0.428  0.253  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_file['target']\n",
    "drops = ['id', 'target']\n",
    "X = train_file\n",
    "X.drop(drops, inplace=True, axis=1)\n",
    "X.head()\n",
    "\n",
    "#vif = pd.DataFrame()\n",
    "#vif[\"features\"] = train_file.columns\n",
    "#vif[\"vif_score\"] = [variance_inflation_factor(train_file.values, i) for i in range(train_file.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td>-1.033</td>\n",
       "      <td>-1.595</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.687</td>\n",
       "      <td>1.291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-2.628</td>\n",
       "      <td>-0.845</td>\n",
       "      <td>2.078</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>2.132</td>\n",
       "      <td>0.609</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776</td>\n",
       "      <td>0.914</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>1.347</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.578</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.203</td>\n",
       "      <td>1.356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.683</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>-1.133</td>\n",
       "      <td>-3.138</td>\n",
       "      <td>0.281</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.750</td>\n",
       "      <td>0.509</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.835</td>\n",
       "      <td>-0.476</td>\n",
       "      <td>1.428</td>\n",
       "      <td>-0.701</td>\n",
       "      <td>-2.009</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>0.167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.351</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>-0.737</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.135</td>\n",
       "      <td>-1.327</td>\n",
       "      <td>2.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.556</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.512</td>\n",
       "      <td>-1.419</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.567</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>0.255</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>0.916</td>\n",
       "      <td>2.411</td>\n",
       "      <td>1.053</td>\n",
       "      <td>-1.601</td>\n",
       "      <td>-1.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.754</td>\n",
       "      <td>-0.245</td>\n",
       "      <td>1.173</td>\n",
       "      <td>-1.623</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.781</td>\n",
       "      <td>-1.763</td>\n",
       "      <td>-1.432</td>\n",
       "      <td>-0.930</td>\n",
       "      <td>...</td>\n",
       "      <td>2.184</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.186</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9  \\\n",
       "0  0.500 -1.033 -1.595  0.309 -0.714  0.502  0.535 -0.129 -0.687  1.291   \n",
       "1  0.776  0.914 -0.494  1.347 -0.867  0.480  0.578 -0.313  0.203  1.356   \n",
       "2  1.750  0.509 -0.057  0.835 -0.476  1.428 -0.701 -2.009 -1.378  0.167   \n",
       "3 -0.556 -1.855 -0.682  0.578  1.592  0.512 -1.419  0.722  0.511  0.567   \n",
       "4  0.754 -0.245  1.173 -1.623  0.009  0.370  0.781 -1.763 -1.432 -0.930   \n",
       "\n",
       "   ...      290    291    292    293    294    295    296    297    298    299  \n",
       "0  ...   -0.088 -2.628 -0.845  2.078 -0.277  2.132  0.609 -0.104  0.312  0.979  \n",
       "1  ...   -0.683 -0.066  0.025  0.606 -0.353 -1.133 -3.138  0.281 -0.625 -0.761  \n",
       "2  ...   -0.094  0.351 -0.607 -0.737 -0.031  0.701  0.976  0.135 -1.327  2.463  \n",
       "3  ...   -0.336 -0.787  0.255 -0.031 -0.836  0.916  2.411  1.053 -1.601 -1.529  \n",
       "4  ...    2.184 -1.090  0.216  1.186 -0.143  0.322 -0.068 -0.156 -1.153  0.825  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_tst = ['id']\n",
    "X_tst = test_file\n",
    "X_tst.drop(drop_tst, inplace = True, axis=1)\n",
    "X_tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination - keep 200 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive feature elimination\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "# create the RFE model for the svm classifier \n",
    "# and select attributes\n",
    "rfe = RFE(logreg, 200)\n",
    "rfe1 = rfe.fit(X, y)\n",
    "# print summaries for the selection of attributes\n",
    "#print(rfe1.support_)\n",
    "#print(rfe1.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only the important variables from above in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '3' '4' '5' '8' '9' '11' '13' '14' '15' '16' '17' '18' '20' '24'\n",
      " '25' '28' '30' '33' '34' '36' '41' '42' '43' '44' '45' '46' '49' '50'\n",
      " '51' '52' '53' '56' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '78' '79' '80' '81' '82' '83' '84' '89' '90' '91' '92'\n",
      " '93' '94' '95' '96' '97' '98' '99' '100' '101' '102' '103' '105' '106'\n",
      " '107' '108' '110' '111' '112' '113' '114' '115' '116' '117' '118' '119'\n",
      " '120' '123' '125' '127' '128' '129' '130' '131' '132' '133' '134' '135'\n",
      " '137' '138' '141' '143' '145' '146' '147' '148' '149' '150' '151' '152'\n",
      " '153' '155' '156' '157' '160' '161' '164' '165' '168' '170' '171' '174'\n",
      " '176' '180' '183' '187' '188' '189' '190' '192' '193' '194' '195' '196'\n",
      " '197' '198' '199' '201' '202' '203' '206' '207' '208' '209' '211' '212'\n",
      " '214' '215' '216' '217' '218' '219' '220' '221' '224' '225' '226' '227'\n",
      " '228' '229' '230' '234' '236' '237' '239' '241' '242' '244' '245' '246'\n",
      " '247' '249' '251' '252' '257' '258' '259' '261' '262' '263' '264' '266'\n",
      " '268' '269' '271' '272' '274' '275' '276' '277' '279' '280' '281' '283'\n",
      " '285' '288' '289' '291' '292' '294' '295' '298' '299']\n"
     ]
    }
   ],
   "source": [
    "features_bool = np.array(rfe1.support_)\n",
    "features = np.array(X.columns)\n",
    "result = features[features_bool]\n",
    "print(result)\n",
    "X = X[result]\n",
    "\n",
    "#keeping the same in the tst dataset\n",
    "X_tst = X_tst[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 200)\n",
      "(19750, 200)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) #(250, 200)\n",
    "#X_tst.head()\n",
    "print(X_tst.shape) #(19750, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 70-30 split goes as \n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the model (using the default parameters)\n",
    "log_clf = LogisticRegression()\n",
    "# fit the model with data\n",
    "log_clf.fit(X_train,y_train)\n",
    "y_pred=log_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  4],\n",
       "       [ 7, 40]], dtype=int64)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8533333333333334\n",
      "Precision: 0.9090909090909091\n",
      "Recall: 0.851063829787234\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9792746, 0.0207254])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_pred\n",
    "#log_clf.predict_proba(X_train)[1][0]\n",
    "log_clf.predict_proba(X_train)[1][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8, 20],\n",
       "       [11, 36]], dtype=int64)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5866666666666667\n",
      "Precision: 0.6428571428571429\n",
      "Recall: 0.7659574468085106\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf = SVC(kernel='linear', C = 1)  \n",
    "svc_clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = svc_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  8],\n",
       "       [ 2, 45]], dtype=int64)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8666666666666667\n",
      "Precision: 0.8490566037735849\n",
      "Recall: 0.9574468085106383\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_clf = Lasso(alpha=0.03, tol=0.01, selection='random', random_state=42)\n",
    "#lasso_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splits = 10\n",
    "# folds = RepeatedStratifiedKFold(n_splits=splits, n_repeats=20, random_state=42)\n",
    "# oof_preds = np.zeros(X_train.shape[0])\n",
    "# sub_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# for fold_, (trn_, val_) in enumerate(folds.split(X_train, y_train)):\n",
    "#     trn_x, trn_y = X_train.iloc[trn_], y_train[trn_]\n",
    "#     val_x, val_y = X_train.iloc[val_], y_train[val_]\n",
    "\n",
    "# model = RFECV(lasso_clf, step=1, cv=(splits - 1))\n",
    "# model.fit(X_train, y_train)\n",
    "# oof_preds[val_] = model.predict(val_x).clip(0, 1)\n",
    "# sub_preds += model.predict(X_test).clip(0, 1) / splits / 20 #folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "# print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID search for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-273-cd1af008f02c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_1 = { 'bootstrap': [True],\n",
    "           'max_depth': [80, 90, 100, 110],\n",
    "        'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "   'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 1000]\n",
    "           }\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf, param_grid = grid_1, n_jobs=-1, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6571428571428571"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 90,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfclf1 = RandomForestClassifier(bootstrap= True, max_depth= 80, max_features= 3, min_samples_leaf= 3, min_samples_split= 8,\n",
    "                               n_estimators= 100)\n",
    "rfclf1.fit(X_train, y_train)\n",
    "y_pred = rfclf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6266666666666667\n",
      "Precision: 0.6266666666666667\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID search wrapper - precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': [3, 5, 10], \n",
    "    'n_estimators' : [100, 300],\n",
    "    'max_depth': [3, 5, 15, 25],\n",
    "    'max_features': [3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': metrics.make_scorer(metrics.precision_score),\n",
    "    'recall_score': metrics.make_scorer(metrics.recall_score),\n",
    "    'accuracy_score': metrics.make_scorer(metrics.accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(y_test, y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for precision_score\n",
      "{'max_depth': 15, 'max_features': 20, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\n",
      "Confusion matrix of Random Forest optimized for precision_score on the test data:\n",
      "     pred_neg  pred_pos\n",
      "neg         1        27\n",
      "pos         0        47\n"
     ]
    }
   ],
   "source": [
    "grid_search_clf = grid_search_wrapper(refit_score='precision_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_precision_score</th>\n",
       "      <th>mean_test_recall_score</th>\n",
       "      <th>mean_test_accuracy_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.680</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.669</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.669</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.661</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.669</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.663</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.659</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.663</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_precision_score  mean_test_recall_score  \\\n",
       "70                      0.675                   0.982   \n",
       "90                      0.669                   0.974   \n",
       "36                      0.661                   1.000   \n",
       "66                      0.660                   0.991   \n",
       "72                      0.659                   0.991   \n",
       "\n",
       "    mean_test_accuracy_score param_max_depth param_max_features  \\\n",
       "70                     0.680              15                 20   \n",
       "90                     0.669              25                 20   \n",
       "36                     0.669               5                 10   \n",
       "66                     0.663              15                 20   \n",
       "72                     0.663              25                  3   \n",
       "\n",
       "   param_min_samples_split param_n_estimators  \n",
       "70                      10                100  \n",
       "90                       3                100  \n",
       "36                       3                100  \n",
       "66                       3                100  \n",
       "72                       3                100  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search_clf.cv_results_)\n",
    "results = results.sort_values(by='mean_test_precision_score', ascending=False)\n",
    "results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID search wrapper - recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='recall_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(y_test, y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for recall_score\n",
      "{'max_depth': 3, 'max_features': 3, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "\n",
      "Confusion matrix of Random Forest optimized for recall_score on the test data:\n",
      "     pred_neg  pred_pos\n",
      "neg         0        28\n",
      "pos         0        47\n"
     ]
    }
   ],
   "source": [
    "grid_search_clf = grid_search_wrapper(refit_score='recall_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_precision_score</th>\n",
       "      <th>mean_test_recall_score</th>\n",
       "      <th>mean_test_accuracy_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.670</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.666</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.674</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.663</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.657</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.659</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.663</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_precision_score  mean_test_recall_score  \\\n",
       "60                      0.670                   1.000   \n",
       "94                      0.666                   1.000   \n",
       "70                      0.660                   0.991   \n",
       "90                      0.660                   0.974   \n",
       "67                      0.659                   0.991   \n",
       "\n",
       "    mean_test_accuracy_score param_max_depth param_max_features  \\\n",
       "60                     0.680              15                 10   \n",
       "94                     0.674              25                 20   \n",
       "70                     0.663              15                 20   \n",
       "90                     0.657              25                 20   \n",
       "67                     0.663              15                 20   \n",
       "\n",
       "   param_min_samples_split param_n_estimators  \n",
       "60                       3                100  \n",
       "94                      10                100  \n",
       "70                      10                100  \n",
       "90                       3                100  \n",
       "67                       3                300  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search_clf.cv_results_)\n",
    "results = results.sort_values(by='mean_test_precision_score', ascending=False)\n",
    "results[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_accuracy_score', 'param_max_depth', 'param_max_features', 'param_min_samples_split', 'param_n_estimators']].round(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust decison threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_scores = grid_search_clf.predict_proba(X_test)[:, 1]\n",
    "# for classifiers with decision_function, this achieves similar results\n",
    "# y_scores = classifier.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, r, thresholds = metrics.precision_recall_curve(y_test, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "def precision_recall_threshold(p, r, thresholds, t=0.5):\n",
    "    \"\"\"\n",
    "    plots the precision recall curve and shows the current value for each\n",
    "    by identifying the classifier's threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate new class predictions based on the adjusted_classes\n",
    "    # function above and view the resulting confusion matrix.\n",
    "    y_pred_adj = adjusted_classes(y_scores, t)\n",
    "    print(pd.DataFrame(metrics.confusion_matrix(y_test, y_pred_adj),\n",
    "                       columns=['pred_neg', 'pred_pos'], \n",
    "                       index=['neg', 'pos']))\n",
    "    \n",
    "    # plot the curve\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Precision and Recall curve ^ = current threshold\")\n",
    "    plt.step(r, p, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(r, p, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    plt.ylim([0.5, 1.01]);\n",
    "    plt.xlim([0.5, 1.01]);\n",
    "    plt.xlabel('Recall');\n",
    "    plt.ylabel('Precision');\n",
    "    \n",
    "    # plot the current threshold on the line\n",
    "    close_default_clf = np.argmin(np.abs(thresholds - t))\n",
    "    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',\n",
    "            markersize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pred_neg  pred_pos\n",
      "neg        10        18\n",
      "pos         4        43\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAHwCAYAAAC/hfaiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDHVJkgrx/wEe2RONwxMxSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269aa22c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_recall_threshold(p, r, thresholds, 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7256838905775076\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, auc_thresholds = metrics.roc_curve(y_test, y_scores)\n",
    "print(metrics.auc(fpr, tpr)) # AUC of ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAH8CAYAAADIRzbZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3KibmZk1CTfqZmZmTcKNupmZWZP4PzzQjplN6skkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x269a8c2b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(fpr, tpr, 'recall_optimized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_clf = ensemble.GradientBoostingClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred = tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 15],\n",
       "       [ 3, 44]], dtype=int64)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.7457627118644068\n",
      "Recall: 0.9361702127659575\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble of SVM and logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('logistic', log_clf))\n",
    "estimators.append(('gbm', tree_clf))\n",
    "estimators.append(('randomforest', rfclf1))\n",
    "estimators.append(('svm', svc_clf))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8880000000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.9 , 0.88, 0.86, 0.94, 0.86])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = cross_val_score(ensemble, X, y, cv=5)\n",
    "print(results.mean())\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for unknown dataset using ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the ID to predictions and extracting final submissions file\n",
    "\n",
    "#pd.concat(columns = {'y_pred':'target'}, inplace = True)\n",
    "#rsrch = pd.get_dummies(.Research, prefix = 'rsrch')\n",
    "\n",
    "test_file = pd.read_csv(\"C:/Users/mruna/Desktop/Kaggle/Dont_overfit/test.csv\")\n",
    "\n",
    "Submission = test_file\n",
    "Submission['target'] = y_pred\n",
    "Submission = Submission[['id', 'target']]\n",
    "\n",
    "Submission.to_csv ('C:/Users/mruna/Desktop/Kaggle/Dont_overfit/Submission.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file = pd.concat([test_file, y_pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
